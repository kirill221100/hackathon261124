{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !ollama pull gemma2:9b\n",
    "# !ollama pull nextfire/paraphrase-multilingual-minilm:l12-v2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.ollama_client import OllamaClient\n",
    "from scripts.text_indexer import TextIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: The color of the fox is **brown**. \n",
      "\n",
      "This is stated directly in the sentence: \"The quick brown fox...\"  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from scripts.ollama_client import OllamaClient\n",
    "from scripts.text_indexer import TextIndexer\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http import models\n",
    "\n",
    "ollama_client = OllamaClient(\n",
    "    host=\"localhost\",\n",
    "    llm_port=11434,\n",
    "    temperature = 0.0,\n",
    "    model_name=\"gemma2:9b\",\n",
    "    embedding_model=\"nextfire/paraphrase-multilingual-minilm:l12-v2\"\n",
    ")\n",
    "\n",
    "# Initialize QdrantClient\n",
    "qdrant_client = QdrantClient(host=\"localhost\", port=6333)\n",
    "\n",
    "# Sample document\n",
    "text = \"The quick brown fox jumps over the lazy dog.\"\n",
    "\n",
    "# Create an indexer\n",
    "indexer = TextIndexer(text=text, ollama_client=ollama_client, qdrant_client=qdrant_client)\n",
    "\n",
    "# Process and index the document\n",
    "indexer.process_file()\n",
    "\n",
    "# User query\n",
    "query = \"What is the color of the fox?\"\n",
    "\n",
    "# Generate query embedding\n",
    "query_vector = ollama_client.get_embeddings([query])[0]\n",
    "\n",
    "# Retrieve top K relevant chunks from Qdrant\n",
    "top_k = 5\n",
    "retrieved_chunks = indexer.retrieve(query_vector=query_vector, top_k=top_k)\n",
    "\n",
    "# Combine relevant chunks with query and generate response using Ollama\n",
    "relevant_text = \"\\n\".join([chunk[\"payload\"][\"text\"] for chunk in retrieved_chunks])\n",
    "final_prompt = f\"Given the following information:\\n{relevant_text}\\n\\nAnswer the question: {query}\"\n",
    "\n",
    "response = ollama_client.generate(final_prompt)\n",
    "print(\"Response:\", response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
